building pipeline:
    creatin the github repo and cloning it(add a foler experiments)
    add src folder along with the all the components(run them individually)
    inside the src 
        make a data_ingestion.py file which will see 
        preprocessing.py
        feature_engineering.py
        model_training.py
        model_evaluation.py

    add data,models,report dir to .gitignore file
    commit the changes in the github

set the DVC pipeline(without params)
    create a dvc.yaml file and stage to it 
    "dvc init" then "dvc repro" to test the pipeline automation(check the "dvc dag")
    now git add,commit,push
        
setting the dvc pipeline with the params
    create a params.yaml file
    add the params setup
    do the dvc repro again to check the pipeline along with the params
    now git add,commit,push

experimenting with the DVC:
    pip install dvclive
    add the dvclive codeblock
    do the "dvc exp run" it will create the new dvc.yaml file(is not already there) and dvclive directory
    run "dvc exp show" to see the experiments or use dvc extension in VSCode
    do "dvc exp remove {exp-name}" to remove the exp (optional)
    dvc exp apply {exp-name} to reproduce the desired exp
    change the params and re-run the code(reproduce new exp)
    now git add,commit and push


adding the remote storage system(aws,GCP,etc)
-->  login to aws 
-->  create a IAMUser
-->  create S3 bucket(give name to it example:- yt-mlops-dvc-s3)
-->  "pip install dvc[s3]"
-->  "pip install awscli"
-->  "aws configure" - on terminal(give the access id and access secret key)
-->  "dvc remote add -d dvcstore s3://<name of the bucket>" ("dvc remote add -d --force dvcstore <remote-storage-url>" to override the config)
-->    once done check the config file in the .dvc dir it will show the core and remote url

--> dvc commit,push the exp outcome you want to keep(if the desired outcome is not reproduced through dvc reproduce it with the "dvc exp apply<expname>"command)

